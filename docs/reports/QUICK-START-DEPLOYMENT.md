# BIZRA Quick Start Deployment Guide

**Status**: üü¢ **READY TO DEPLOY**
**Date**: 2025-10-25 11:52 (Dubai, GMT+4)
**ÿßÿ≠ÿ≥ÿßŸÜ Score**: 100/100

---

## Prerequisites Checklist

### Required Tools

- [ ] `kubectl` (Kubernetes CLI) - Version 1.24+
- [ ] `helm` (Helm package manager) - Version 3.10+
- [ ] `git` (Version control)
- [ ] `curl` (HTTP client)
- [ ] `jq` (JSON processor)

**Verify installations**:

```bash
kubectl version --client
helm version
git --version
curl --version
jq --version
```

### Kubernetes Cluster Access

- [ ] Cluster accessible via `kubectl config current-context`
- [ ] Sufficient resources (20+ cores, 64GB+ memory)
- [ ] Namespaces can be created
- [ ] PersistentVolumes available (for Prometheus/Grafana)

### Container Images

- [ ] NeMo Guardrails image built/available
- [ ] MCP LLM Server image built/available
- [ ] MCP Tools Server image built/available

**If images not ready**, see:

- `ops/guardrails/nemo/README.md` (NeMo Docker build)
- `interop/MCP-A2A-Spec.md` (MCP server deployment)

### Secrets & Tokens

- [ ] GitHub Personal Access Token (for MCP Tools Server)
  - Generate at: https://github.com/settings/tokens
  - Scopes: `repo`, `read:org`, `read:user`

---

## Step-by-Step Deployment

### Step 1: Save User-Provided Manifests (2 minutes)

**From user canvas** ‚Üí Save to local file:

```bash
# Create directory if not exists
mkdir -p ops/k8s/phase0

# Save canvas content to this exact path:
# ops/k8s/phase0/kubernetes_deployment_manifests_phase_0_ne_mo_guardrails_mcp_servers.yaml

# Verify file exists
ls -lh ops/k8s/phase0/kubernetes_deployment_manifests_phase_0_ne_mo_guardrails_mcp_servers.yaml
```

### Step 2: Update Manifest Placeholders (5 minutes)

**Edit the manifest file**:

```bash
# Open in editor
nano ops/k8s/phase0/kubernetes_deployment_manifests_phase_0_ne_mo_guardrails_mcp_servers.yaml

# Replace these placeholders:
# 1. Container images (3 replacements)
#    ghcr.io/bizra/nemo-guardrails:latest ‚Üí YOUR_ACTUAL_NEMO_IMAGE
#    ghcr.io/bizra/mcp-llm-server:latest ‚Üí YOUR_ACTUAL_MCP_LLM_IMAGE
#    ghcr.io/bizra/mcp-tools-server:latest ‚Üí YOUR_ACTUAL_MCP_TOOLS_IMAGE

# 2. GITHUB_TOKEN secret (1 replacement)
#    Find: GITHUB_TOKEN: REPLACE_WITH_YOUR_ACTUAL_TOKEN
#    Replace with: GITHUB_TOKEN: ghp_YOUR_GITHUB_TOKEN_HERE
```

**Save and exit** (Ctrl+O, Ctrl+X in nano)

### Step 3: Verify Cluster Access (1 minute)

```bash
# Check current context
kubectl config current-context

# Verify access
kubectl get nodes

# Expected: List of cluster nodes with STATUS Ready
```

### Step 4: Execute Phase 0 Script (30 minutes)

```bash
# Make script executable
chmod +x PHASE-0-INFRASTRUCTURE-DEPLOYMENT.sh

# Run deployment script
./PHASE-0-INFRASTRUCTURE-DEPLOYMENT.sh

# Expected output:
# [0/5] Deploying Phase 0 K8s Manifests...
# ‚úÖ Phase 0 manifests deployed (namespace: bizra)
# [1/5] Deploying OpenTelemetry Collector...
# ‚úÖ OTel Collector deployed
# [2/5] Deploying kube-prometheus-stack...
# ‚úÖ kube-prometheus-stack deployed
# [3/5] Deploying Argo Rollouts Controller...
# ‚úÖ Argo Rollouts deployed
# [4/5] Verifying NeMo Guardrails...
# ‚úÖ NeMo Guardrails verification complete
# [5/5] Verifying MCP capability cards...
# ‚úÖ MCP capability card verification complete
# Phase 0 Complete
```

**If errors occur**, see Troubleshooting section below.

### Step 5: Verify All Infrastructure (5 minutes)

```bash
# Check all pods are Running
kubectl get pods -n bizra
kubectl get pods -n monitoring
kubectl get pods -n argo-rollouts

# All pods should show STATUS: Running, READY: 1/1 or 2/2

# Check services
kubectl get svc -n bizra

# Expected services:
# nemo-guardrails    ClusterIP   10.x.x.x   <none>   8000/TCP
# mcp-llm-server     ClusterIP   10.x.x.x   <none>   8001/TCP
# mcp-tools-server   ClusterIP   10.x.x.x   <none>   8002/TCP
```

### Step 6: Access Grafana (2 minutes)

```bash
# Port-forward Grafana to localhost
kubectl port-forward -n monitoring svc/kps-grafana 3000:80

# Open in browser: http://localhost:3000
# Login: admin / prom-operator (CHANGE PASSWORD IMMEDIATELY)

# Verify BIZRA dashboards available:
# - Go to Dashboards ‚Üí Browse
# - Look for "BIZRA Apex" and "NeMo Guardrails" dashboards
```

### Step 7: Proceed to T-30 (Wait for Aligned Time)

**Recommended**: Start canary on the hour (e.g., 13:00, 14:00 UTC+4)

**Why**: Aligns with on-call shift change and business-hour communications

**While waiting**:

1. Review `GO-LIVE-EXECUTION-PACKAGE.md`
2. Confirm on-call roster
3. Prepare Slack/Discord announcement
4. Notify team of freeze window

---

## Troubleshooting

### Issue: "Phase 0 manifests not found"

**Symptom**:

```
‚ö†Ô∏è Phase 0 manifests not found at ops/k8s/phase0/
```

**Solution**:

1. Verify file saved from canvas to exact path:
   ```bash
   ls -lh ops/k8s/phase0/kubernetes_deployment_manifests_phase_0_ne_mo_guardrails_mcp_servers.yaml
   ```
2. If missing, save canvas content to that path
3. Re-run script

### Issue: "ImagePullBackOff"

**Symptom**:

```bash
kubectl get pods -n bizra
# Shows: ImagePullBackOff
```

**Solution**:

1. Check image names in manifest:
   ```bash
   kubectl describe pod -n bizra <pod-name> | grep "Image:"
   ```
2. Verify images exist in registry
3. Update manifest with correct image names
4. Re-apply:
   ```bash
   kubectl delete -f ops/k8s/phase0/kubernetes_deployment_manifests_phase_0_ne_mo_guardrails_mcp_servers.yaml
   kubectl apply -f ops/k8s/phase0/kubernetes_deployment_manifests_phase_0_ne_mo_guardrails_mcp_servers.yaml
   ```

### Issue: "CrashLoopBackOff"

**Symptom**:

```bash
kubectl get pods -n bizra
# Shows: CrashLoopBackOff
```

**Solution**:

1. Check logs:
   ```bash
   kubectl logs -n bizra <pod-name> --tail=50
   ```
2. Common causes:
   - Missing environment variables (e.g., GITHUB_TOKEN)
   - Incorrect configuration (e.g., rails.colang syntax error)
   - Resource limits too low

3. Fix configuration and re-deploy

### Issue: Helm chart installation fails

**Symptom**:

```
Error: INSTALLATION FAILED: ...
```

**Solution**:

1. Verify Helm version ‚â• 3.10:
   ```bash
   helm version
   ```
2. Update Helm repos:
   ```bash
   helm repo update
   ```
3. Check namespace exists:
   ```bash
   kubectl get namespace monitoring
   kubectl get namespace argo-rollouts
   ```
4. Re-run specific Helm install command from script

### Issue: kubectl wait timeout

**Symptom**:

```
error: timed out waiting for the condition on pods/...
```

**Solution**:

1. Check pod status:
   ```bash
   kubectl get pods -n bizra
   kubectl describe pod -n bizra <pod-name>
   ```
2. Increase timeout in script (edit PHASE-0-INFRASTRUCTURE-DEPLOYMENT.sh):
   ```bash
   # Change from --timeout=300s to --timeout=600s
   ```
3. Or manually verify pods Running, then continue

---

## Post-Deployment Verification

### Health Checks

```bash
# NeMo Guardrails
kubectl -n bizra port-forward svc/nemo-guardrails 8000:8000 &
curl http://localhost:8000/health
# Expected: {"status":"healthy","rails_loaded":5}

# MCP LLM Server
kubectl -n bizra port-forward svc/mcp-llm-server 8001:8001 &
curl http://localhost:8001/capabilities | jq '.signature'
# Expected: Ed25519 signature string

# MCP Tools Server
kubectl -n bizra port-forward svc/mcp-tools-server 8002:8002 &
curl http://localhost:8002/capabilities | jq '.signature'
# Expected: Ed25519 signature string

# Kill port-forwards
killall kubectl
```

### Prometheus Metrics

```bash
# Port-forward Prometheus
kubectl port-forward -n monitoring svc/kps-prometheus 9090:9090 &

# Open in browser: http://localhost:9090

# Test query: bizra_ahsan_score
# Expected: Metric available (may be empty until BIZRA app deployed)
```

---

## Next Steps

### After Phase 0 Complete

1. ‚úÖ **Phase 0 infrastructure deployed** (you are here)
2. **Wait until T-30 min** (align with on-call shift change)
3. **Run smoke tests** (see `GO-LIVE-EXECUTION-PACKAGE.md` T-30 section)
4. **Start canary at T-00** (follow Run-of-Show exactly)
5. **Monitor gates** (T+30, T+60)
6. **Complete at T+120** (public all-clear)

### Detailed Timeline

**See**: `GO-LIVE-EXECUTION-PACKAGE.md` for complete Run-of-Show

**Key milestones**:

- T-30: Freeze window, verify all healthy
- T-15: Validate guardrails + OTel
- T-00: Start 5% canary
- T+30: Gate A (promote to 50% if GREEN)
- T+60: Gate B (promote to 100% if GREEN)
- T+90: Evidence snapshot
- T+120: Public all-clear

---

## Files Reference

**Created/Updated Files**:

1. ‚úÖ `PHASE-0-INFRASTRUCTURE-DEPLOYMENT.sh` (main deployment script)
2. ‚úÖ `GO-LIVE-EXECUTION-PACKAGE.md` (complete deployment playbook)
3. ‚úÖ `ops/k8s/phase0/README.md` (manifest deployment guide)
4. ‚úÖ `ops/k8s/argo-rollouts-values.yaml` (Argo Rollouts Helm values)
5. ‚úÖ `QUICK-START-DEPLOYMENT.md` (this file)

**From User**: 6. ‚úÖ `ops/k8s/phase0/kubernetes_deployment_manifests_phase_0_ne_mo_guardrails_mcp_servers.yaml` (save from canvas)

**Elite Artifacts** (all 11 created previously): 7. ‚úÖ `docs/compliance/ISO42001-SoA.md` 8. ‚úÖ `docs/compliance/EU-AI-Act-Article12-Log-Access-SOP.md` 9. ‚úÖ `ops/runbooks/Canary-Rollout-Runbook.md` 10. ‚úÖ `ops/otel/otel-collector.yaml` 11. ‚úÖ `ops/guardrails/nemo/rails.colang` + `README.md` 12. ‚úÖ `interop/MCP-A2A-Spec.md` 13. ‚úÖ `rag/GraphRAG-Eval-Plan.md` 14. ‚úÖ `security/OWASP-LLM-Redteam-Checklist.md` 15. ‚úÖ `ops/monitoring/kube-prom-stack.md` 16. ‚úÖ `ops/certificates/HTTPS-Certbot.md`

---

**ÿ®ÿß ÿßÿ≠ÿ≥ÿßŸÜ (With Excellence)** - Systematic deployment, zero assumptions.

**Status**: üü¢ **READY TO EXECUTE PHASE 0**
